import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings("ignore")

# å…¨å±€å¸¸é‡ï¼ˆé€‚é…ä½ çš„æ•°æ®é›†ï¼šç§»é™¤Protocolï¼Œç”¨16ç»´ç‰¹å¾ï¼‰
FEATURE_DIM = 16  # åŸ17ç»´å»æ‰Protocolï¼Œæ”¹ä¸º16ç»´
PCA_DIM = 12      # PCAé™ç»´åç»´åº¦
ATTACK_TYPES = [
    "Benign", "DoS_Hulk", "DoS_GoldenEye",
    "PortScan", "DDoS", "BruteForce"  # å¤šæ”»å‡»ç±»å‹æ”¯æŒ
]

def load_cicids2017(data_path):
    """åŠ è½½CICIDS2017æ•°æ®é›†ï¼ˆé€‚é…ä½ çš„åˆ—åï¼‰"""
    import os
    csv_files = [f for f in os.listdir(data_path) if f.endswith(".csv")]
    df_list = []
    total_raw_rows = 0  # æ–°å¢ï¼šç»Ÿè®¡åŸå§‹æ•°æ®æ€»è¡Œæ•°
    for csv in csv_files:
        df = pd.read_csv(os.path.join(data_path, csv), low_memory=False)
        total_raw_rows += len(df)  # ç´¯åŠ æ¯ä¸ªCSVçš„è¡Œæ•°
        df_list.append(df)
    df = pd.concat(df_list, ignore_index=True)

    # 1. æ•°æ®æ¸…æ´—
    clean_before_rows = len(df)  # æ¸…æ´—å‰æ€»è¡Œæ•°
    df = df.dropna()
    df = df.replace([np.inf, -np.inf], np.nan).dropna()
    clean_after_rows = len(df)  # æ¸…æ´—åæ€»è¡Œæ•°

    # 2. åŒ¹é…æ ‡ç­¾åˆ—ï¼ˆä½ çš„åˆ—åæ˜¯' Label'ï¼‰
    df.rename(columns={' Label': "Label"}, inplace=True)

    # 3. æ ‡ç­¾æ˜ å°„
    df["Label"] = df["Label"].str.strip()
    df["Label"] = df["Label"].str.replace("BENIGN", "Benign")
    df["Label"] = df["Label"].str.replace("DoS Hulk", "DoS_Hulk")
    df["Label"] = df["Label"].str.replace("DoS GoldenEye", "DoS_GoldenEye")
    df["Label"] = df["Label"].str.replace("Portscan", "PortScan")
    df["Label"] = df["Label"].str.replace("DDOS", "DDoS")
    label_filtered_rows = len(df)  # æ ‡ç­¾è¿‡æ»¤å‰è¡Œæ•°
    df = df[df["Label"].isin(ATTACK_TYPES)]
    final_rows = len(df)  # æœ€ç»ˆç”¨äºè®­ç»ƒçš„æ•°æ®è¡Œæ•°

    # 4. ç‰¹å¾é€‰æ‹©ï¼ˆé€‚é…ä½ çš„åˆ—åï¼Œç§»é™¤Protocolï¼‰
    core_features_mapping = {
        "Dst Port": [" Destination Port"],
        "Flow Duration": [" Flow Duration"],
        "Total Fwd Packets": [" Total Fwd Packets"],
        "Total Backward Packets": [" Total Backward Packets"],
        "Total Length of Fwd Packets": ["Total Length of Fwd Packets"],
        "Total Length of Bwd Packets": [" Total Length of Bwd Packets"],
        "Fwd Packet Length Max": [" Fwd Packet Length Max"],
        "Fwd Packet Length Min": [" Fwd Packet Length Min"],
        "Fwd Packet Length Mean": [" Fwd Packet Length Mean"],
        "Bwd Packet Length Max": ["Bwd Packet Length Max"],
        "Bwd Packet Length Min": [" Bwd Packet Length Min"],
        "Bwd Packet Length Mean": [" Bwd Packet Length Mean"],
        "Flow Bytes/s": ["Flow Bytes/s"],
        "Flow Packets/s": [" Flow Packets/s"],
        "Fwd IAT Mean": [" Fwd IAT Mean"],
        "Bwd IAT Mean": [" Bwd IAT Mean"]
    }

    # åŒ¹é…ç‰¹å¾åˆ—
    actual_features = []
    for target, possible_names in core_features_mapping.items():
        for name in possible_names:
            if name in df.columns:
                actual_features.append(name)
                print(f"âœ… åŒ¹é…ç‰¹å¾ï¼šç›®æ ‡'{target}' â†’ å®é™…åˆ—å'{name}'")
                break

    # é€‰æ‹©ç‰¹å¾åˆ—+æ ‡ç­¾åˆ—
    df = df[actual_features + ["Label"]]
    df.columns = list(core_features_mapping.keys()) + ["Label"]

    # 5. æ ‡ç­¾ç¼–ç 
    le = LabelEncoder()
    df["Label_Enc"] = le.fit_transform(df["Label"])

    # æ–°å¢ï¼šè¿”å›æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    data_stats = {
        "åŸå§‹æ•°æ®æ€»è¡Œæ•°": total_raw_rows,
        "åˆå¹¶åæœªæ¸…æ´—è¡Œæ•°": clean_before_rows,
        "æ¸…æ´—åè¡Œæ•°ï¼ˆå»ç©º/å»æ— ç©·ï¼‰": clean_after_rows,
        "æ ‡ç­¾è¿‡æ»¤å‰è¡Œæ•°": label_filtered_rows,
        "æœ€ç»ˆæœ‰æ•ˆè¡Œæ•°ï¼ˆå«ç›®æ ‡æ”»å‡»ç±»å‹ï¼‰": final_rows,
        "ç‰¹å¾ç»´åº¦": len(actual_features)
    }
    return df, le, data_stats  # æ–°å¢è¿”å›ç»Ÿè®¡ä¿¡æ¯

def add_differential_privacy(features, epsilon=1.0, delta=1e-5):
    """æ·»åŠ å·®åˆ†éšç§ä¿æŠ¤"""
    sensitivity = np.max(np.linalg.norm(features, axis=1))
    sigma = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon
    noise = np.random.normal(0, sigma, features.shape)
    return features + noise

def preprocess_pipeline(data_path, save_path="./preprocessed_data/"):
    """å®Œæ•´é¢„å¤„ç†æµæ°´çº¿"""
    # 1. åŠ è½½æ•°æ®ï¼ˆæ¥æ”¶ç»Ÿè®¡ä¿¡æ¯ï¼‰
    df, le, data_stats = load_cicids2017(data_path)
    X = df.drop(["Label", "Label_Enc"], axis=1).values
    y = df["Label_Enc"].values

    # 2. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 3. PCAé™ç»´
    pca = PCA(n_components=PCA_DIM)
    X_pca = pca.fit_transform(X_scaled)

    # 4. å·®åˆ†éšç§ä¿æŠ¤
    X_dp = add_differential_privacy(X_pca)

    # 5. åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_dp, y, test_size=0.3, random_state=42, stratify=y
    )

    # 6. ä¿å­˜ç»“æœ
    import os
    os.makedirs(save_path, exist_ok=True)
    np.save(os.path.join(save_path, "X_train.npy"), X_train)
    np.save(os.path.join(save_path, "X_test.npy"), X_test)
    np.save(os.path.join(save_path, "y_train.npy"), y_train)
    np.save(os.path.join(save_path, "y_test.npy"), y_test)
    np.save(os.path.join(save_path, "label_encoder.npy"), le.classes_)

    joblib.dump(scaler, os.path.join(save_path, "scaler.pkl"))
    joblib.dump(pca, os.path.join(save_path, "pca.pkl"))

    # æ–°å¢ï¼šæ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æ•°æ®é›†æ€»é‡ç»Ÿè®¡ï¼š")
    for key, value in data_stats.items():
        print(f"  - {key}ï¼š{value:,}")  # åƒåˆ†ä½æ ¼å¼åŒ–ï¼Œä¾¿äºé˜…è¯»

    # æ–°å¢ï¼šæ‰“å°å„æ”»å‡»ç±»å‹çš„æ•°é‡åˆ†å¸ƒ
    print(f"\nğŸ“ˆ å„æ”»å‡»ç±»å‹æ•°é‡åˆ†å¸ƒï¼š")
    label_count = df["Label"].value_counts()
    for label, count in label_count.items():
        percentage = (count / len(df)) * 100
        print(f"  - {label}ï¼š{count:,} æ¡ï¼ˆ{percentage:.2f}%ï¼‰")

    print(f"\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼š")
    print(f"  - è®­ç»ƒé›†ï¼š{X_train.shape} | æµ‹è¯•é›†ï¼š{X_test.shape}")
    print(f"  - æ”»å‡»ç±»å‹ï¼š{le.classes_}")
    print(f"  - ä¿å­˜è·¯å¾„ï¼š{save_path}")
    return X_train, X_test, y_train, y_test, scaler, pca, le

if __name__ == "__main__":
    preprocess_pipeline(
        data_path=r"E:\IntelliJ IDEA 2024.2.4\Network Security\PythonIDS - å‰¯æœ¬\CICIDS2017",
        save_path="./preprocessed_data/"
    )