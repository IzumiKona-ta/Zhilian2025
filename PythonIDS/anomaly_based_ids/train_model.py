import sys
import os
import torch
# é¡¹ç›®è·¯å¾„é…ç½®ï¼ˆå¦‚æœä»£ç å’Œä¾èµ–åœ¨åŒä¸€ç›®å½•ï¼Œå¯ä»¥æ³¨é‡Šæ‰ï¼‰
# PROJECT_PATH = "/home/test/ids_project"
# sys.path.append(PROJECT_PATH)
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import logging
import multiprocessing

# Opacuså·®åˆ†éšç§æ”¯æŒ
try:
    from opacus import PrivacyEngine
    from opacus.validators import ModuleValidator
    OPACUS_AVAILABLE = True
    logger_opacus = logging.getLogger(__name__)
except ImportError:
    OPACUS_AVAILABLE = False
    logger_opacus = logging.getLogger(__name__)
    logger_opacus.warning("âš ï¸ Opacusæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡†è®­ç»ƒï¼ˆæ— å·®åˆ†éšç§ï¼‰")

from ids_common import (
    TransformerEncoder, Generator, Discriminator,
    SEQ_LEN, PCA_DIM, NUM_CLASSES, LATENT_DIM,
    DEVICE, PREPROCESS_DIR, MODEL_DIR
)

# ========== å…¨å±€é…ç½®ï¼ˆæ‹‰æ»¡å¼ºåº¦ï¼‰ ==========
TEST_MODE = False
# é…ç½®æ—¥å¿—ï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - [è®­ç»ƒæ—¥å¿—] - %(message)s",
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆä¼šè¢«nohupé‡å®šå‘ï¼‰
        logging.FileHandler('train_detailed.log', encoding='utf-8', mode='a')  # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)
logger = logging.getLogger(__name__)
# ç¡®ä¿loggerè¾“å‡ºä¸ä¼šè¢«ç¼“å†²
logger.setLevel(logging.INFO)
# è®¾ç½®multiprocessingå¯åŠ¨æ–¹å¼ï¼ˆé¿å…èµ„æºæ³„æ¼è­¦å‘Šï¼‰
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥é”™è¯¯
    pass

# å¤šGPUæ£€æµ‹ï¼ˆæ‹‰æ»¡CUDAä¼˜åŒ–ï¼‰
def auto_detect_multi_gpu():
    if not torch.cuda.is_available():
        raise RuntimeError("âŒ æœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼")
    gpu_count = torch.cuda.device_count()
    logger.info(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡ï¼š")
    for gpu_idx in range(gpu_count):
        props = torch.cuda.get_device_properties(gpu_idx)
        logger.info(f"  - GPU {gpu_idx}ï¼š{props.name}ï¼ˆæ˜¾å­˜ï¼š{props.total_memory//1024//1024}GBï¼‰")
    # å¼€å¯CUDAå¼‚æ­¥è®¡ç®—+TF32ä¼˜åŒ–ï¼ˆæ‹‰æ»¡ç®—åŠ›ï¼‰
    torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å·ç§¯ç®—æ³•
    torch.backends.cudnn.deterministic = False  # ç¦ç”¨ç¡®å®šæ€§ä»¥è·å¾—æœ€å¿«é€Ÿåº¦
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_tf32 = True
    # å¢åŠ cudnn workspace sizeä»¥æé«˜æ€§èƒ½
    torch.backends.cudnn.max_workspace_size = 2 * 1024 * 1024 * 1024  # 2GB
    # æ³¨æ„ï¼šä¸è¦è®¾ç½®set_device(0)ï¼Œè®©DataParallelè‡ªåŠ¨ç®¡ç†æ‰€æœ‰GPU
    return gpu_count

GPU_COUNT = auto_detect_multi_gpu()

# è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¹³è¡¡è®­ç»ƒé€Ÿåº¦å’Œæ•ˆæœï¼‰
EPOCHS = 2 if TEST_MODE else 60  # è®­ç»ƒè½®æ¬¡60è½®
# ã€é€Ÿåº¦ä¼˜åŒ–ã€‘é€‚åº¦å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œå¹³è¡¡é€Ÿåº¦å’Œæ•ˆæœ
BATCH_SIZE = 1024 * GPU_COUNT    # å•å¡1024ï¼Œæ€»æ‰¹æ¬¡4096ï¼ˆæé«˜GPUåˆ©ç”¨ç‡ï¼Œæ‹‰æ»¡æ˜¾å­˜ï¼‰
# å¦‚æœé€Ÿåº¦è¿˜æ˜¯å¤ªæ…¢ï¼Œå¯ä»¥é™åˆ° 512 * GPU_COUNTï¼ˆå›åˆ°åŸæ¥çš„2048ï¼‰
LEARNING_RATE = 1e-4             # å­¦ä¹ ç‡ï¼ˆæå‡åˆ°1e-4ï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ï¼Œå¹³è¡¡ç¨³å®šæ€§å’Œæ•ˆç‡ï¼‰
# ã€é€Ÿåº¦ä¼˜åŒ–ã€‘é™ä½è®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼ŒåŠ å¿«é€Ÿåº¦
CRITIC_ITERATIONS = 3            # åˆ¤åˆ«å™¨è®­ç»ƒæ¬¡æ•°ï¼ˆé™åˆ°3ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼‰
GENERATOR_ITERATIONS = 3         # ç”Ÿæˆå™¨è®­ç»ƒæ¬¡æ•°ï¼ˆé™åˆ°3ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼‰
# ã€é€Ÿåº¦ä¼˜åŒ–ã€‘é™ä½ç”Ÿæˆå™¨æ•°æ®é‡ï¼Œå¤§å¹…åŠ å¿«é€Ÿåº¦
FAKE_SAMPLE_MULTIPLE = 2         # ç”Ÿæˆæ ·æœ¬æ•°é‡ = çœŸå®æ ·æœ¬æ•°é‡ * 2ï¼ˆé™åˆ°2ï¼Œå¤§å¹…åŠ å¿«é€Ÿåº¦ï¼‰
# å¦‚æœè¿˜æ˜¯å¤ªæ…¢ï¼Œå¯ä»¥é™åˆ° 2ï¼ˆå›åˆ°åŸæ¥çš„é…ç½®ï¼‰
CLASS_LOSS_WEIGHT = 1.0          # åˆ†ç±»æŸå¤±æƒé‡ï¼ˆæå‡åˆ°1.0ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¼šåˆ†ç±»ï¼‰

# Opacuså·®åˆ†éšç§å‚æ•°
# ã€é‡è¦ã€‘Opacusä¸GANè®­ç»ƒå­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œå»ºè®®æš‚æ—¶ç¦ç”¨
# å¦‚æœé‡åˆ° "Per sample gradient is not initialized" é”™è¯¯ï¼Œè¯·è®¾ç½® USE_DP_TRAINING = False
USE_DP_TRAINING = False          # æ˜¯å¦å¯ç”¨å·®åˆ†éšç§è®­ç»ƒï¼ˆå»ºè®®æš‚æ—¶ç¦ç”¨ï¼‰
NOISE_MULTIPLIER = 1.0           # å™ªå£°ä¹˜æ•°ï¼ˆæ§åˆ¶éšç§é¢„ç®—æ¶ˆè€—é€Ÿåº¦ï¼‰
MAX_GRAD_NORM = 1.0              # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆL2èŒƒæ•°ï¼‰
DELTA = 1e-5                     # å·®åˆ†éšç§çš„deltaå‚æ•°ï¼ˆé€šå¸¸è®¾ä¸º1/æ•°æ®é›†å¤§å°ï¼‰

# ========== æ•°æ®é›†ç±»ï¼ˆæ‹‰æ»¡æ•°æ®åŠ è½½ï¼‰ ==========
class TrafficDataset(Dataset):
    def __init__(self, X, y, seq_len=SEQ_LEN):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)
        self.seq_len = seq_len

    def __len__(self):
        return len(self.X) - self.seq_len + 1

    def __getitem__(self, idx):
        x_seq = self.X[idx:idx+self.seq_len]
        y_label = self.y[idx+self.seq_len-1]
        return x_seq, y_label

# ========== TransEC-GANè®­ç»ƒç±»ï¼ˆæ ¸å¿ƒä¼˜åŒ–ç”Ÿæˆå™¨æ•°æ®é‡ï¼‰ ==========
class TransEC_GAN(nn.Module):
    def __init__(self):
        super().__init__()
        self.generator = Generator().to(DEVICE)
        self.discriminator = Discriminator().to(DEVICE)
        # ç”Ÿæˆå™¨å§‹ç»ˆå¯ä»¥ä½¿ç”¨DataParallelï¼ˆ4ä¸ªGPUï¼‰
        # æ˜ç¡®æŒ‡å®šdevice_idsç¡®ä¿ä½¿ç”¨æ‰€æœ‰GPU
        if GPU_COUNT > 1:
            self.generator = nn.DataParallel(
                self.generator,
                device_ids=list(range(GPU_COUNT)),  # æ˜ç¡®æŒ‡å®šä½¿ç”¨æ‰€æœ‰GPU
                output_device=0  # ä¸»GPUä¸ºGPU 0
            )
        # åˆ¤åˆ«å™¨ï¼šå¦‚æœå¯ç”¨Opacusï¼Œä¸ä½¿ç”¨DataParallelï¼ˆOpacusä¸æ”¯æŒï¼‰
        # å¦‚æœç¦ç”¨Opacusï¼Œå¯ä»¥ä½¿ç”¨DataParallelï¼ˆ4ä¸ªGPUï¼‰
        # æ³¨æ„ï¼šåœ¨__init__ä¸­ç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ï¼ˆæ­¤æ—¶è¿˜æœªè¿›å…¥å‡½æ•°ä½œç”¨åŸŸï¼‰
        if GPU_COUNT > 1 and not (USE_DP_TRAINING and OPACUS_AVAILABLE):
            self.discriminator = nn.DataParallel(
                self.discriminator,
                device_ids=list(range(GPU_COUNT)),  # æ˜ç¡®æŒ‡å®šä½¿ç”¨æ‰€æœ‰GPU
                output_device=0  # ä¸»GPUä¸ºGPU 0
            )
        # å…¼å®¹æ—§ç‰ˆæœ¬çš„GradScaler
        self.scaler = torch.amp.GradScaler(enabled=True)
        self.g_optim = optim.Adam(self.generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))
        self.d_optim = optim.Adam(self.discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))
        
        # ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°å¤„ç†æ•°æ®ä¸å¹³è¡¡
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼šBenignæ ·æœ¬å¤šï¼Œæƒé‡å°ï¼›æ”»å‡»æ ·æœ¬å°‘ï¼Œæƒé‡å¤§
        # æƒé‡ = æ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * è¯¥ç±»æ ·æœ¬æ•°)
        self.class_weights = self._calculate_class_weights()
        
        # Opacusè¦æ±‚ä½¿ç”¨reduction='none'çš„æŸå¤±å‡½æ•°ä»¥æ”¯æŒper-sampleæ¢¯åº¦
        if USE_DP_TRAINING and OPACUS_AVAILABLE:
            self.class_criterion = nn.CrossEntropyLoss(
                weight=self.class_weights.to(DEVICE),
                reduction='none'  # per-sampleæ¢¯åº¦
            )
        else:
            self.class_criterion = nn.CrossEntropyLoss(weight=self.class_weights.to(DEVICE))
        
        # Opacus PrivacyEngineï¼ˆå°†åœ¨è®­ç»ƒæ—¶åˆå§‹åŒ–ï¼‰
        self.privacy_engine = None
    
    def _calculate_class_weights(self):
        """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ•°æ®ä¸å¹³è¡¡"""
        try:
            labels = np.load(os.path.join(PREPROCESS_DIR, "y_train.npy"))
            unique, counts = np.unique(labels, return_counts=True)
            total_samples = len(labels)
            
            # åˆ›å»ºé•¿åº¦ä¸ºNUM_CLASSESçš„æƒé‡å¼ é‡ï¼ˆç¡®ä¿ä¸æ¨¡å‹è¾“å‡ºç»´åº¦åŒ¹é…ï¼‰
            weights = torch.ones(NUM_CLASSES, dtype=torch.float32)
            
            # è®¡ç®—æƒé‡ï¼šæ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * è¯¥ç±»æ ·æœ¬æ•°)
            # åªæ›´æ–°æ•°æ®é›†ä¸­å­˜åœ¨çš„ç±»åˆ«
            for i, label in enumerate(unique):
                label_idx = int(label)
                if 0 <= label_idx < NUM_CLASSES:
                    class_count = counts[i]
                    weights[label_idx] = total_samples / (NUM_CLASSES * class_count)
            
            # è®°å½•æƒé‡ä¿¡æ¯
            weight_dict = {int(label): float(weights[int(label)]) for label in unique if 0 <= int(label) < NUM_CLASSES}
            logger.info(f"ğŸ“Š ç±»åˆ«æƒé‡è®¡ç®—å®Œæˆï¼ˆå…±{NUM_CLASSES}ä¸ªç±»åˆ«ï¼‰ï¼š{weight_dict}")
            logger.info(f"   æ•°æ®é›†ä¸­å­˜åœ¨çš„ç±»åˆ«ï¼š{unique.tolist()}")
            
            return weights
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•è®¡ç®—ç±»åˆ«æƒé‡ï¼Œä½¿ç”¨é»˜è®¤æƒé‡ï¼š{e}")
            return torch.ones(NUM_CLASSES, dtype=torch.float32)

    def generate_fake(self, batch_size, labels):
        """ç”ŸæˆFAKE_SAMPLE_MULTIPLEå€æ•°é‡çš„fakeæ ·æœ¬ï¼Œå¤§å¹…æé«˜ç”Ÿæˆæ•°æ®é‡ï¼ˆå¢å¼ºç”Ÿæˆå™¨è®­ç»ƒå¼ºåº¦ï¼‰"""
        z = torch.randn(batch_size * FAKE_SAMPLE_MULTIPLE, LATENT_DIM, device=DEVICE)
        labels_expanded = labels.repeat(FAKE_SAMPLE_MULTIPLE)
        labels_onehot = torch.nn.functional.one_hot(labels_expanded, NUM_CLASSES).float()
        return self.generator(z, labels_onehot)

    def train_step(self, real_x, real_labels):
        batch_size = real_x.shape[0]
        d_loss = None
        g_loss = None
        real_class = None

        # é˜¶æ®µ1ï¼šè®­ç»ƒåˆ¤åˆ«å™¨
        for _ in range(CRITIC_ITERATIONS):
            self.discriminator.train()
            self.generator.eval()
            # ã€å…³é”®ä¿®å¤ã€‘Opacusè¦æ±‚æ¯æ¬¡zero_gradåï¼Œforwardå’Œbackwardå¿…é¡»åŒ¹é…
            # ç¡®ä¿åœ¨æ¯æ¬¡è¿­ä»£å¼€å§‹æ—¶æ¸…ç†æ‰€æœ‰æ¿€æ´»
            self.d_optim.zero_grad()

            # ã€å…³é”®ä¿®å¤ã€‘Opacusä¸æ”¯æŒFP16ï¼Œåœ¨Opacusæ¨¡å¼ä¸‹ç¦ç”¨AMP
            # Opacusçš„per-sampleæ¢¯åº¦è®¡ç®—éœ€è¦FP32ç²¾åº¦
            use_amp = not (USE_DP_TRAINING and OPACUS_AVAILABLE)
            
            if USE_DP_TRAINING and OPACUS_AVAILABLE:
                # ã€Opacusæ¨¡å¼ã€‘åªå¯¹realæ•°æ®è®¡ç®—per-sampleæ¢¯åº¦ï¼ˆå·®åˆ†éšç§ä¿æŠ¤ï¼‰
                # fakeæ•°æ®ä½¿ç”¨æ ‡å‡†æ¢¯åº¦ï¼ˆä¸éœ€è¦éšç§ä¿æŠ¤ï¼‰
                
                # ç”Ÿæˆfakeæ•°æ®ï¼ˆæ‰¹æ¬¡å¤§å°ï¼šbatch_sizeï¼Œä¸realæ•°æ®ä¸€è‡´ï¼‰
                fake_labels = torch.randint(1, NUM_CLASSES, (batch_size,), device=DEVICE)
                z = torch.randn(batch_size, LATENT_DIM, device=DEVICE)
                labels_onehot = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()
                fake_x = self.generator(z, labels_onehot)
                
                with torch.amp.autocast('cuda', enabled=False):  # Opacuséœ€è¦FP32
                    # ã€å…³é”®ä¿®å¤ã€‘åˆ†åˆ«å¤„ç†realå’Œfakeæ•°æ®
                    # realæ•°æ®ï¼šéœ€è¦per-sampleæ¢¯åº¦ï¼ˆç”¨äºå·®åˆ†éšç§ï¼‰
                    real_pred, real_class = self.discriminator(real_x)
                    
                    # è®¡ç®—realæ•°æ®çš„per-sampleæŸå¤±
                    # WGANæŸå¤±ï¼šåˆ¤åˆ«å™¨å¸Œæœ›real_predå¤§ï¼Œæ‰€ä»¥æŸå¤±æ˜¯ -real_predï¼ˆper-sampleï¼‰
                    d_loss_real_per_sample = -real_pred.squeeze()  # [batch_size]
                    
                    # åˆ†ç±»æŸå¤±ï¼ˆper-sampleï¼Œreduction='none'å·²è®¾ç½®ï¼‰
                    d_loss_class_per_sample = self.class_criterion(real_class, real_labels)  # [batch_size]
                    class_weights_gpu = self.class_weights.to(DEVICE)
                    real_labels_gpu = real_labels.to(DEVICE)
                    d_loss_class_weighted = d_loss_class_per_sample * class_weights_gpu[real_labels_gpu]  # [batch_size]
                    
                    # realæ•°æ®çš„per-sampleæ€»æŸå¤±ï¼ˆè¿™æ˜¯Opacuséœ€è¦çš„æ ¼å¼ï¼‰
                    d_loss_real_per_sample_total = d_loss_real_per_sample + CLASS_LOSS_WEIGHT * d_loss_class_weighted  # [batch_size]
                    
                    # fakeæ•°æ®ï¼šä½¿ç”¨æ ‡å‡†æŸå¤±ï¼ˆä¸éœ€è¦per-sampleæ¢¯åº¦ï¼‰
                    fake_pred, _ = self.discriminator(fake_x.detach())  # detaché¿å…å½±å“per-sampleæ¢¯åº¦
                    d_loss_fake = torch.mean(fake_pred)  # æ ‡é‡
                    
                    # æ€»æŸå¤±ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
                    d_loss_real_mean = d_loss_real_per_sample_total.mean()  # æ ‡é‡
                    d_loss = d_loss_real_mean + d_loss_fake  # æ€»æŸå¤±
                    
                    # NaNæ£€æµ‹
                    if torch.isnan(d_loss) or torch.isinf(d_loss):
                        logger.warning("âš ï¸ åˆ¤åˆ«å™¨æŸå¤±å€¼å¼‚å¸¸ï¼ˆNaN/Infï¼‰ï¼Œè·³è¿‡æ­¤è¿­ä»£")
                        continue
                
                # ã€å…³é”®ä¿®å¤ã€‘Opacuséœ€è¦per-sampleæŸå¤±è¿›è¡Œbackward
                # è­¦å‘Šï¼šå½“å‰å®ç°å¯èƒ½ä¸å®Œå…¨å…¼å®¹Opacusï¼Œå¦‚æœé‡åˆ°é”™è¯¯ï¼Œè¯·ç¦ç”¨USE_DP_TRAINING
                # 
                # æ–¹æ¡ˆï¼šåªå¯¹realæ•°æ®çš„per-sampleæŸå¤±è¿›è¡Œbackward
                # Opacusä¼šåœ¨backwardæ—¶è‡ªåŠ¨è®¡ç®—per-sample gradients
                # fakeæ•°æ®åœ¨è¿™ä¸ªiterationä¸­ä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼ˆGANè®­ç»ƒå¯èƒ½éœ€è¦è°ƒæ•´ï¼‰
                d_loss_real_per_sample_total.mean().backward()
                
                # æ³¨æ„ï¼šfakeæŸå¤±ä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼Œè¿™å¯èƒ½å¯¼è‡´GANè®­ç»ƒä¸ç¨³å®š
                # å¦‚æœéœ€è¦å®Œæ•´çš„GANè®­ç»ƒï¼Œå»ºè®®ç¦ç”¨Opacusï¼ˆè®¾ç½®USE_DP_TRAINING = Falseï¼‰
                # ã€å…³é”®ä¿®å¤ã€‘å¢å¼ºæ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆä»1.0é™ä½åˆ°0.5ï¼‰
                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)
                # ã€å…³é”®ä¿®å¤ã€‘Opacusè¦æ±‚stepåæ¸…ç†æ¿€æ´»ï¼Œç¡®ä¿ä¸‹æ¬¡è¿­ä»£æ—¶æ¿€æ´»åˆ—è¡¨ä¸ºç©º
                self.d_optim.step()
                # ã€å…³é”®ä¿®å¤ã€‘WGANæƒé‡è£å‰ªï¼šé™åˆ¶åˆ¤åˆ«å™¨æƒé‡åœ¨[-0.01, 0.01]èŒƒå›´å†…ï¼Œé˜²æ­¢æƒé‡çˆ†ç‚¸
                # è¿™æ˜¯WGANçš„æ ‡å‡†åšæ³•ï¼Œå¯ä»¥é˜²æ­¢åˆ¤åˆ«å™¨è¾“å‡ºå€¼è¿‡å¤§
                with torch.no_grad():
                    if hasattr(self.discriminator, '_module'):  # OpacusåŒ…è£…çš„æ¨¡å‹
                        for param in self.discriminator._module.parameters():
                            param.clamp_(-0.1, 0.1)
                    elif isinstance(self.discriminator, nn.DataParallel):
                        for param in self.discriminator.module.parameters():
                            param.clamp_(-0.1, 0.1)
                    else:
                        for param in self.discriminator.parameters():
                            param.clamp_(-0.1, 0.1)
                # ç¡®ä¿æ¿€æ´»è¢«æ¸…ç†ï¼ˆOpacusä¼šåœ¨stepä¸­è‡ªåŠ¨æ¸…ç†ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬æ˜¾å¼æ¸…ç†ï¼‰
                if hasattr(self.discriminator, '_module'):
                    # OpacusåŒ…è£…çš„æ¨¡å‹ï¼Œæ¸…ç†æ¿€æ´»
                    for module in self.discriminator._module.modules():
                        if hasattr(module, 'activations'):
                            module.activations.clear()
            else:
                # ã€ç¨³å®šæ€§ä¿®å¤ã€‘ç¦ç”¨AMPæ··åˆç²¾åº¦ï¼Œä½¿ç”¨FP32é˜²æ­¢æ•°å€¼æº¢å‡ºå¯¼è‡´NaN
                # with torch.amp.autocast('cuda', enabled=use_amp):
                if True:  # å¼ºåˆ¶ä½¿ç”¨FP32
                    real_pred, real_class = self.discriminator(real_x)
                    d_loss_real = -torch.mean(real_pred)
                    
                    d_loss_class_per_sample = self.class_criterion(real_class, real_labels)
                    if d_loss_class_per_sample.dim() > 0:
                        d_loss_class = d_loss_class_per_sample.mean()
                    else:
                        d_loss_class = d_loss_class_per_sample
                    
                    fake_labels = torch.randint(1, NUM_CLASSES, (batch_size,), device=DEVICE)
                    fake_x = self.generate_fake(batch_size, fake_labels)
                    fake_pred, _ = self.discriminator(fake_x.detach())
                    d_loss_fake = torch.mean(fake_pred)
                    
                    d_loss = d_loss_real + d_loss_fake + CLASS_LOSS_WEIGHT * d_loss_class
                    
                    if torch.isnan(d_loss) or torch.isinf(d_loss):
                        logger.warning("âš ï¸ åˆ¤åˆ«å™¨æŸå¤±å€¼å¼‚å¸¸ï¼ˆNaN/Infï¼‰ï¼Œè·³è¿‡æ­¤è¿­ä»£")
                        continue
                
                # self.scaler.scale(d_loss).backward()
                d_loss.backward()  # ç›´æ¥backwardï¼Œä¸ä½¿ç”¨scaler
                # ã€å…³é”®ä¿®å¤ã€‘å¢å¼ºæ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆä»1.0é™ä½åˆ°0.5ï¼‰
                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)
                # self.scaler.step(self.d_optim)
                # self.scaler.update()
                self.d_optim.step()
                # ã€å…³é”®ä¿®å¤ã€‘WGANæƒé‡è£å‰ªï¼šé™åˆ¶åˆ¤åˆ«å™¨æƒé‡åœ¨[-0.1, 0.1]èŒƒå›´å†…ï¼Œé˜²æ­¢æƒé‡çˆ†ç‚¸
                with torch.no_grad():
                    if isinstance(self.discriminator, nn.DataParallel):
                        for param in self.discriminator.module.parameters():
                            param.clamp_(-0.1, 0.1)
                    else:
                        for param in self.discriminator.parameters():
                            param.clamp_(-0.1, 0.1)

        # é˜¶æ®µ2ï¼šè®­ç»ƒç”Ÿæˆå™¨
        for _ in range(GENERATOR_ITERATIONS):
            self.generator.train()
            self.g_optim.zero_grad()
            
            # ã€å…³é”®ä¿®å¤ã€‘Opacusæ¨¡å¼ä¸‹ï¼Œç”Ÿæˆå™¨è®­ç»ƒæ—¶è°ƒç”¨åˆ¤åˆ«å™¨ä¼šå¯¼è‡´æ¿€æ´»è·Ÿè¸ªé—®é¢˜
            # Opacusè¦æ±‚æ¯ä¸ªforwardéƒ½æœ‰å¯¹åº”çš„backwardï¼Œä½†ç”Ÿæˆå™¨çš„backwardä¸ä¼šæ¸…ç†åˆ¤åˆ«å™¨çš„æ¿€æ´»
            # è§£å†³æ–¹æ¡ˆï¼šåœ¨ç”Ÿæˆå™¨è®­ç»ƒæ—¶ï¼Œä½¿ç”¨detach()åˆ†ç¦»åˆ¤åˆ«å™¨è¾“å‡ºï¼Œé¿å…è§¦å‘Opacusçš„æ¿€æ´»è·Ÿè¸ª
            if USE_DP_TRAINING and OPACUS_AVAILABLE:
                # ã€Opacusæ¨¡å¼ã€‘ç”Ÿæˆå™¨è®­ç»ƒæ—¶çš„ç‰¹æ®Šå¤„ç†
                # é—®é¢˜ï¼šOpacusçš„æ¿€æ´»è·Ÿè¸ªæœºåˆ¶ä¸GANè®­ç»ƒæµç¨‹ä¸å…¼å®¹
                # ç”Ÿæˆå™¨è®­ç»ƒæ—¶éœ€è¦è°ƒç”¨åˆ¤åˆ«å™¨ï¼Œä½†Opacusè¦æ±‚æ¯ä¸ªforwardéƒ½æœ‰å¯¹åº”çš„backward
                # è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æœªåŒ…è£…çš„åˆ¤åˆ«å™¨å‰¯æœ¬ï¼ˆæ²¡æœ‰Opacus hookï¼‰ï¼Œé¿å…æ¿€æ´»è·Ÿè¸ªé—®é¢˜
                
                # ç”Ÿæˆfakeæ•°æ®
                fake_x = self.generate_fake(batch_size, real_labels)
                
                # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨æœªåŒ…è£…çš„åˆ¤åˆ«å™¨å‰¯æœ¬ï¼ˆæ²¡æœ‰Opacus hookï¼‰
                # è¿™æ ·ç”Ÿæˆå™¨å¯ä»¥æ­£å¸¸åå‘ä¼ æ’­ï¼Œä½†ä¸ä¼šè§¦å‘Opacusçš„æ¿€æ´»è·Ÿè¸ª
                if hasattr(self, 'discriminator_for_generator') and self.discriminator_for_generator is not None:
                    # åŒæ­¥æƒé‡ï¼ˆä»OpacusåŒ…è£…çš„åˆ¤åˆ«å™¨å¤åˆ¶åˆ°æœªåŒ…è£…çš„å‰¯æœ¬ï¼‰
                    self.discriminator_for_generator.load_state_dict(self.discriminator.state_dict(), strict=False)
                    self.discriminator_for_generator.eval()
                    
                    with torch.amp.autocast('cuda', enabled=False):  # Opacuséœ€è¦FP32
                        fake_pred, _ = self.discriminator_for_generator(fake_x)
                else:
                    # å¦‚æœæ²¡æœ‰å‰¯æœ¬ï¼Œå›é€€åˆ°ä½¿ç”¨åŸå§‹åˆ¤åˆ«å™¨ï¼ˆä½†ä¼šæœ‰æ¿€æ´»è·Ÿè¸ªé—®é¢˜ï¼‰
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°åˆ¤åˆ«å™¨å‰¯æœ¬ï¼Œä½¿ç”¨åŸå§‹åˆ¤åˆ«å™¨ï¼ˆå¯èƒ½æœ‰æ¿€æ´»è·Ÿè¸ªé—®é¢˜ï¼‰")
                    self.discriminator.eval()
                    with torch.amp.autocast('cuda', enabled=False):  # Opacuséœ€è¦FP32
                        fake_pred, _ = self.discriminator(fake_x)
                
                # åªä½¿ç”¨WGANæŸå¤±ï¼ˆç”Ÿæˆå™¨å¸Œæœ›fake_predå¤§ï¼‰
                g_loss = -torch.mean(fake_pred)
                
                if torch.isnan(g_loss) or torch.isinf(g_loss):
                    logger.warning("âš ï¸ ç”Ÿæˆå™¨æŸå¤±å€¼å¼‚å¸¸ï¼ˆNaN/Infï¼‰ï¼Œè·³è¿‡æ­¤è¿­ä»£")
                    continue
                
                # ç›´æ¥è¿›è¡Œç”Ÿæˆå™¨çš„backwardï¼ˆä¸ä¼šè§¦å‘Opacusçš„æ¿€æ´»è·Ÿè¸ªï¼Œå› ä¸ºä½¿ç”¨çš„æ˜¯æœªåŒ…è£…çš„å‰¯æœ¬ï¼‰
                with torch.amp.autocast('cuda', enabled=False):  # Opacuséœ€è¦FP32
                    g_loss.backward()
                
                # ã€å…³é”®ä¿®å¤ã€‘å¢å¼ºæ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆä»1.0é™ä½åˆ°0.5ï¼‰
                torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 1.0)
                self.g_optim.step()
                
                # æ¸…ç†åˆ¤åˆ«å™¨çš„æ¿€æ´»ï¼ˆé€šè¿‡zero_gradï¼‰
                if hasattr(self, 'discriminator_for_generator') and self.discriminator_for_generator is not None:
                    self.discriminator_for_generator.zero_grad()
                self.discriminator.zero_grad()
            else:
                # æ ‡å‡†æ¨¡å¼ï¼šæ­£å¸¸è®­ç»ƒç”Ÿæˆå™¨ï¼ˆåŒ…å«åˆ†ç±»æŸå¤±ï¼‰
                self.discriminator.eval()
                
                # with torch.amp.autocast('cuda'):
                if True:  # å¼ºåˆ¶ä½¿ç”¨FP32
                    fake_x = self.generate_fake(batch_size, real_labels)
                    fake_pred, fake_class = self.discriminator(fake_x)
                    g_loss_fake = -torch.mean(fake_pred)
                    fake_labels_expanded = real_labels.repeat(FAKE_SAMPLE_MULTIPLE)
                    
                    g_loss_class_per_sample = self.class_criterion(fake_class, fake_labels_expanded)
                    if g_loss_class_per_sample.dim() > 0:
                        g_loss_class = g_loss_class_per_sample.mean()
                    else:
                        g_loss_class = g_loss_class_per_sample
                    
                    g_loss = g_loss_fake + CLASS_LOSS_WEIGHT * g_loss_class
                    
                    if torch.isnan(g_loss) or torch.isinf(g_loss):
                        logger.warning("âš ï¸ ç”Ÿæˆå™¨æŸå¤±å€¼å¼‚å¸¸ï¼ˆNaN/Infï¼‰ï¼Œè·³è¿‡æ­¤è¿­ä»£")
                        continue

                    # self.scaler.scale(g_loss).backward()
                    g_loss.backward()
                    # ã€å…³é”®ä¿®å¤ã€‘å¢å¼ºæ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆä»1.0é™ä½åˆ°0.5ï¼‰
                    torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 1.0)
                    # self.scaler.step(self.g_optim)
                    # self.scaler.update()
                    self.g_optim.step()

        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä½¿ç”¨æœ€åä¸€æ¬¡åˆ¤åˆ«å™¨è¾“å‡ºçš„åˆ†ç±»ç»“æœï¼‰
        if real_class is not None:
            real_acc = (real_class.argmax(1) == real_labels).float().mean().item()
        else:
            real_acc = 0.0
        
        # æœ€ç»ˆNaNæ£€æµ‹ï¼šå¦‚æœæŸå¤±å€¼å¼‚å¸¸ï¼Œè¿”å›é»˜è®¤å€¼
        if d_loss is not None and not (torch.isnan(d_loss) or torch.isinf(d_loss)):
            d_loss_val = d_loss.item()
        else:
            d_loss_val = 0.0
            
        if g_loss is not None and not (torch.isnan(g_loss) or torch.isinf(g_loss)):
            g_loss_val = g_loss.item()
        else:
            g_loss_val = 0.0
        
        return {
            "d_loss": d_loss_val,
            "g_loss": g_loss_val,
            "real_acc": real_acc
        }

# ========== è®­ç»ƒå¾ªç¯ï¼ˆæ‹‰æ»¡é…ç½®ï¼‰ ==========
def train_transec_gan():
    global USE_DP_TRAINING  # å£°æ˜å…¨å±€å˜é‡ï¼Œå…è®¸åœ¨å‡½æ•°å†…ä¿®æ”¹
    # åŠ è½½é¢„å¤„ç†æ•°æ®
    X_train = np.load(os.path.join(PREPROCESS_DIR, "X_train.npy"))
    y_train = np.load(os.path.join(PREPROCESS_DIR, "y_train.npy"))
    X_test = np.load(os.path.join(PREPROCESS_DIR, "X_test.npy"))
    y_test = np.load(os.path.join(PREPROCESS_DIR, "y_test.npy"))

    if TEST_MODE:
        X_train = X_train[:len(X_train)//10]
        y_train = y_train[:len(y_train)//10]
        X_test = X_test[:len(X_test)//10]
        y_test = y_test[:len(y_test)//10]
        logger.info("âš ï¸ æµ‹è¯•æ¨¡å¼å·²å¯ç”¨")

    # æ„å»ºæ•°æ®é›†ï¼ˆæ‹‰æ»¡æ•°æ®åŠ è½½å‚æ•°ï¼‰
    train_dataset = TrafficDataset(X_train, y_train)
    test_dataset = TrafficDataset(X_test, y_test)
    # æ•°æ®åŠ è½½å™¨é…ç½®ï¼ˆå‡å°‘num_workersé¿å…èµ„æºæ³„æ¼ï¼‰
    # æ³¨æ„ï¼šnum_workersè¿‡å¤§å¯èƒ½å¯¼è‡´semaphoreæ³„æ¼ï¼Œå»ºè®®è®¾ä¸ºCPUæ ¸å¿ƒæ•°æˆ–æ›´å°
    # oså·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œæ— éœ€é‡å¤å¯¼å…¥
    cpu_count = os.cpu_count() or 8
    # ã€ä¿®å¤èµ„æºæ³„æ¼ã€‘é™ä½num_workersåˆ°8ï¼Œé¿å…semaphoreæ³„æ¼
    # å¯¹äºå¤šGPUè®­ç»ƒï¼Œ8ä¸ªworkerså·²ç»è¶³å¤Ÿï¼Œå¤ªå¤šä¼šå¯¼è‡´èµ„æºæ³„æ¼
    num_workers = min(8, cpu_count // 2) if GPU_COUNT > 1 else min(4, cpu_count // 4)
    if num_workers == 0:
        num_workers = 0  # å¦‚æœè®¡ç®—å‡ºæ¥æ˜¯0ï¼Œä½¿ç”¨0ï¼ˆä¸»è¿›ç¨‹åŠ è½½æ•°æ®ï¼‰
    
    logger.info(f"ğŸ“¦ æ•°æ®åŠ è½½å™¨é…ç½®ï¼šnum_workers={num_workers}, batch_size={BATCH_SIZE}")
    
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        pin_memory=True,  # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
        num_workers=num_workers,
        drop_last=False,
        persistent_workers=False,  # ç¦ç”¨persistent_workersï¼Œé¿å…semaphoreæ³„æ¼
        prefetch_factor=2 if num_workers > 0 else None,  # é¢„åŠ è½½2æ‰¹æ•°æ®ï¼Œé™ä½å†…å­˜å ç”¨
        timeout=30  # è®¾ç½®è¶…æ—¶ï¼Œé¿å…å¡æ­»
    )
    test_loader = DataLoader(
        test_dataset, batch_size=BATCH_SIZE, shuffle=False,
        pin_memory=True,
        num_workers=num_workers,
        drop_last=False,
        persistent_workers=False,  # ç¦ç”¨persistent_workersï¼Œé¿å…semaphoreæ³„æ¼
        prefetch_factor=2 if num_workers > 0 else None,  # é¢„åŠ è½½2æ‰¹æ•°æ®ï¼Œé™ä½å†…å­˜å ç”¨
        timeout=30  # è®¾ç½®è¶…æ—¶ï¼Œé¿å…å¡æ­»
    )

    model = TransEC_GAN()
    best_acc = 0.0
    os.makedirs(MODEL_DIR, exist_ok=True)

    # Opacuså·®åˆ†éšç§é›†æˆï¼ˆä»…å¯¹åˆ¤åˆ«å™¨ï¼‰
    if USE_DP_TRAINING and OPACUS_AVAILABLE:
        logger.info("ğŸ”’ å¯ç”¨Opacuså·®åˆ†éšç§è®­ç»ƒ...")
        logger.info(f"   âš ï¸ æ³¨æ„ï¼šOpacusä¸æ”¯æŒDataParallelï¼Œåˆ¤åˆ«å™¨å°†ä½¿ç”¨å•GPUè®­ç»ƒ")
        logger.info(f"   âœ… ç”Ÿæˆå™¨ä»å¯ä½¿ç”¨{GPU_COUNT}ä¸ªGPUå¹¶è¡Œè®­ç»ƒ")
        try:
            # ç¡®ä¿åˆ¤åˆ«å™¨æ²¡æœ‰DataParallelåŒ…è£…ï¼ˆOpacusä¸æ”¯æŒï¼‰
            if isinstance(model.discriminator, nn.DataParallel):
                # å¦‚æœè¢«DataParallelåŒ…è£…äº†ï¼Œå…ˆè·å–åŸå§‹æ¨¡å‹
                original_discriminator = model.discriminator.module
            else:
                original_discriminator = model.discriminator
            
            # ä½¿ç”¨ModuleValidatorä¿®å¤æ¨¡å‹ç»“æ„
            original_discriminator = ModuleValidator.fix(original_discriminator)
            original_discriminator = original_discriminator.to(DEVICE)
            
            # ã€å…³é”®ä¿®å¤ã€‘ModuleValidator.fix()ä¿®æ”¹äº†æ¨¡å‹ç»“æ„ï¼Œéœ€è¦é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
            # ä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹å‚æ•°åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ï¼Œç¡®ä¿å‚æ•°åŒ¹é…
            model.d_optim = optim.Adam(original_discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))
            
            # åˆ›å»ºPrivacyEngineï¼ˆOpacusä¸æ”¯æŒDataParallelï¼Œå¿…é¡»å•GPUï¼‰
            model.privacy_engine = PrivacyEngine()
            model.discriminator, model.d_optim, train_loader = model.privacy_engine.make_private(
                module=original_discriminator,
                optimizer=model.d_optim,
                data_loader=train_loader,
                noise_multiplier=NOISE_MULTIPLIER,
                max_grad_norm=MAX_GRAD_NORM,
                poisson_sampling=False,  # æ”¯æŒæ¢¯åº¦ç´¯ç§¯
            )
            logger.info("âœ… Opacus PrivacyEngineå·²åˆå§‹åŒ–")
            logger.info(f"   å™ªå£°ä¹˜æ•°ï¼š{NOISE_MULTIPLIER}")
            logger.info(f"   æ¢¯åº¦è£å‰ªï¼š{MAX_GRAD_NORM}")
            logger.info(f"   Deltaï¼š{DELTA}")
            
            # ã€å…³é”®ä¿®å¤ã€‘åˆ›å»ºæœªåŒ…è£…çš„åˆ¤åˆ«å™¨å‰¯æœ¬ï¼Œç”¨äºç”Ÿæˆå™¨è®­ç»ƒ
            # Opacusçš„æ¿€æ´»è·Ÿè¸ªæœºåˆ¶ä¸GANè®­ç»ƒæµç¨‹ä¸å…¼å®¹
            # ç”Ÿæˆå™¨è®­ç»ƒæ—¶éœ€è¦è°ƒç”¨åˆ¤åˆ«å™¨ï¼Œä½†Opacusè¦æ±‚æ¯ä¸ªforwardéƒ½æœ‰å¯¹åº”çš„backward
            # è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ä¸€ä¸ªæœªåŒ…è£…çš„åˆ¤åˆ«å™¨å‰¯æœ¬ï¼ˆæ²¡æœ‰Opacus hookï¼‰ï¼Œé¿å…æ¿€æ´»è·Ÿè¸ªé—®é¢˜
            from ids_common import Discriminator
            # ModuleValidatorå·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œç›´æ¥ä½¿ç”¨
            if not OPACUS_AVAILABLE:
                raise ImportError("Opacusæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºåˆ¤åˆ«å™¨å‰¯æœ¬")
            model.discriminator_for_generator = Discriminator().to(DEVICE)
            model.discriminator_for_generator = ModuleValidator.fix(model.discriminator_for_generator)
            model.discriminator_for_generator = model.discriminator_for_generator.to(DEVICE)
            # å¤åˆ¶æƒé‡ï¼ˆä½†ä¸å¤åˆ¶Opacus hookï¼‰
            model.discriminator_for_generator.load_state_dict(model.discriminator.state_dict(), strict=False)
            model.discriminator_for_generator.eval()  # å§‹ç»ˆå¤„äºevalæ¨¡å¼ï¼Œä¸æ›´æ–°å‚æ•°
            logger.info("âœ… å·²åˆ›å»ºæœªåŒ…è£…çš„åˆ¤åˆ«å™¨å‰¯æœ¬ï¼Œç”¨äºç”Ÿæˆå™¨è®­ç»ƒ")
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            logger.error(f"âŒ Opacusåˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š\n{error_trace}")
            logger.warning("âš ï¸ å°†ä½¿ç”¨æ ‡å‡†è®­ç»ƒï¼ˆæ— å·®åˆ†éšç§ï¼‰")
            USE_DP_TRAINING = False
            model.privacy_engine = None
    else:
        if not OPACUS_AVAILABLE:
            logger.warning("âš ï¸ Opacusæœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒ")
        else:
            logger.info("â„¹ï¸ å·®åˆ†éšç§è®­ç»ƒå·²ç¦ç”¨")

    # æ‰“å°æ‹‰æ»¡é…ç½®ä¿¡æ¯
    logger.info(f"ğŸš€ {GPU_COUNT}å¼ RTX 5880æ‹‰æ»¡è®­ç»ƒå¯åŠ¨ï¼š")
    single_card_batch = BATCH_SIZE // GPU_COUNT
    logger.info(f"  - æ€»æ‰¹æ¬¡ï¼š{BATCH_SIZE}ï¼ˆå•å¡{single_card_batch} Ã— {GPU_COUNT}å¡ï¼‰")
    logger.info(f"  - ç”Ÿæˆå™¨ï¼šä½¿ç”¨{GPU_COUNT}ä¸ªGPUå¹¶è¡Œè®­ç»ƒï¼ˆDataParallelï¼Œdevice_ids={list(range(GPU_COUNT))}ï¼‰")
    if USE_DP_TRAINING and OPACUS_AVAILABLE:
        logger.info(f"  - åˆ¤åˆ«å™¨ï¼šä½¿ç”¨1ä¸ªGPUè®­ç»ƒï¼ˆOpacuså·®åˆ†éšç§è¦æ±‚ï¼Œå›ºå®šåœ¨GPU 0ï¼‰")
    else:
        logger.info(f"  - åˆ¤åˆ«å™¨ï¼šä½¿ç”¨{GPU_COUNT}ä¸ªGPUå¹¶è¡Œè®­ç»ƒï¼ˆDataParallelï¼Œdevice_ids={list(range(GPU_COUNT))}ï¼‰")
    logger.info(f"  - ç”Ÿæˆæ ·æœ¬å€æ•°ï¼š{FAKE_SAMPLE_MULTIPLE}å€ï¼ˆçœŸå®æ ·æœ¬Ã—{FAKE_SAMPLE_MULTIPLE}ï¼Œå¤§å¹…å¢å¼ºç”Ÿæˆå™¨è®­ç»ƒï¼‰")
    logger.info(f"  - è®­ç»ƒè½®æ¬¡ï¼š{EPOCHS}è½®")
    logger.info(f"  - å­¦ä¹ ç‡ï¼š{LEARNING_RATE}")
    logger.info(f"  - æ•°æ®åŠ è½½ï¼šnum_workers={num_workers} + persistent_workers")
    if USE_DP_TRAINING and OPACUS_AVAILABLE:
        logger.info(f"  - å·®åˆ†éšç§ï¼šå¯ç”¨ï¼ˆnoise_multiplier={NOISE_MULTIPLIER}, max_grad_norm={MAX_GRAD_NORM}ï¼‰")

    for epoch in range(EPOCHS):
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        total_metrics = {"d_loss": 0, "g_loss": 0, "real_acc": 0}
        total_samples = 0

        for real_x, real_labels in train_bar:
            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆDataParallelä¼šè‡ªåŠ¨åˆ†å‘åˆ°å¤šä¸ªGPUï¼‰
            real_x = real_x.to(DEVICE, non_blocking=True)  # non_blockingåŠ é€Ÿæ•°æ®ä¼ è¾“
            real_labels = real_labels.to(DEVICE, non_blocking=True)
            batch_samples = real_x.shape[0]
            total_samples += batch_samples

            metrics = model.train_step(real_x, real_labels)
            for k, v in metrics.items():
                total_metrics[k] += v * batch_samples

            train_bar.set_postfix({
                "d_loss": f"{metrics['d_loss']:.4f}",
                "g_loss": f"{metrics['g_loss']:.4f}",
                "acc": f"{metrics['real_acc']:.4f}"
            })

        avg_d_loss = total_metrics["d_loss"] / total_samples
        avg_g_loss = total_metrics["g_loss"] / total_samples
        avg_acc = total_metrics["real_acc"] / total_samples

        # æµ‹è¯•é›†è¯„ä¼°
        test_acc = 0.0
        total_test_samples = 0
        model.discriminator.eval()
        with torch.no_grad():
            for test_x, test_labels in test_loader:
                test_x = test_x.to(DEVICE, non_blocking=True)
                test_labels = test_labels.to(DEVICE, non_blocking=True)
                test_samples = test_x.shape[0]
                total_test_samples += test_samples
                _, test_class = model.discriminator(test_x)
                test_acc += (test_class.argmax(1) == test_labels).float().sum().item()
        test_acc /= total_test_samples

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            
            # è·å–åˆ¤åˆ«å™¨state_dictï¼ˆå¤„ç†DataParallelå’ŒOpacusåŒ…è£…ï¼‰
            if isinstance(model.discriminator, nn.DataParallel):
                disc_state_dict = model.discriminator.module.state_dict()
            elif hasattr(model.discriminator, '_module'):  # OpacusåŒ…è£…çš„æ¨¡å‹
                disc_state_dict = model.discriminator._module.state_dict()
            else:
                disc_state_dict = model.discriminator.state_dict()
            
            # è·å–ç”Ÿæˆå™¨state_dictï¼ˆå¤„ç†DataParallelï¼‰
            if isinstance(model.generator, nn.DataParallel):
                gen_state_dict = model.generator.module.state_dict()
            else:
                gen_state_dict = model.generator.state_dict()
            
            # ä¿å­˜æ¨¡å‹checkpoint
            checkpoint = {
                "generator_state_dict": gen_state_dict,
                "discriminator_state_dict": disc_state_dict,
                "g_optim_state_dict": model.g_optim.state_dict(),
                "d_optim_state_dict": model.d_optim.state_dict(),
                "label_classes": np.load(os.path.join(PREPROCESS_DIR, "label_encoder.npy"), allow_pickle=True),
                "epoch": epoch + 1,
                "best_acc": best_acc,
            }
            
            # ä¿å­˜éšç§é¢„ç®—ä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨Opacusï¼‰
            if USE_DP_TRAINING and OPACUS_AVAILABLE and model.privacy_engine is not None:
                try:
                    epsilon = model.privacy_engine.get_epsilon(delta=DELTA)
                    checkpoint["privacy_budget"] = {
                        "epsilon": float(epsilon),
                        "delta": DELTA,
                        "noise_multiplier": NOISE_MULTIPLIER,
                        "max_grad_norm": MAX_GRAD_NORM,
                        "training_steps": (epoch + 1) * len(train_loader),
                    }
                    logger.info(f"ğŸ’¾ ä¿å­˜æ¨¡å‹ï¼Œå½“å‰éšç§é¢„ç®—ï¼šÎµ={epsilon:.2f}, Î´={DELTA}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ— æ³•è®¡ç®—éšç§é¢„ç®—ï¼š{e}")
            
            torch.save(checkpoint, os.path.join(MODEL_DIR, "best_model_4x5880_max.pth"))

        # è®°å½•éšç§é¢„ç®—ï¼ˆå¦‚æœä½¿ç”¨Opacusï¼‰
        epsilon_info = ""
        if USE_DP_TRAINING and OPACUS_AVAILABLE and model.privacy_engine is not None:
            try:
                epsilon = model.privacy_engine.get_epsilon(delta=DELTA)
                epsilon_info = f" | Îµ={epsilon:.2f}"
            except Exception as e:
                epsilon_info = f" | Îµ=è®¡ç®—å¤±è´¥"
        
        logger.info(
            f"Epoch {epoch+1} | "
            f"d_loss: {avg_d_loss:.4f} | "
            f"g_loss: {avg_g_loss:.4f} | "
            f"train_acc: {avg_acc:.4f} | "
            f"test_acc: {test_acc:.4f} | "
            f"best_acc: {best_acc:.4f}"
            f"{epsilon_info}"
        )

    logger.info(f"âœ… 4å¼ RTX 5880æ‹‰æ»¡è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³ï¼š{MODEL_DIR}")
    return model

if __name__ == "__main__":
    train_transec_gan()