#!/usr/bin/env python3
"""
DP-WGAN æ¨¡å‹ç¦»çº¿è¯„ä¼°è„šæœ¬

åŠŸèƒ½ï¼š
- åŠ è½½é¢„å¤„ç†å¥½çš„æµ‹è¯•æ•°æ®
- åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†ç»„ä»¶
- è®¡ç®—åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ (Precision, Recall, F1)
- è®¡ç®— ROC æ›²çº¿å’Œ AUC
- ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–
- ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š (JSON å’Œ Markdown æ ¼å¼)
"""

import argparse
import json
import logging
import os
from datetime import datetime
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.metrics import (classification_report, confusion_matrix,
                             roc_auc_score, roc_curve)
from torch.utils.data import DataLoader
from tqdm import tqdm

from ids_common import (DEVICE, PREPROCESS_DIR, MODEL_DIR, load_model, logger, SEQ_LEN)

# å…¨å±€å˜é‡ï¼šæ ‡ç­¾åˆ—è¡¨ï¼ˆåœ¨ load_evaluation_data ä¸­åŠ è½½ï¼‰
_labels_cache = None

def get_label_name(idx: int) -> str:
    """æ ¹æ®ç´¢å¼•è·å–æ ‡ç­¾åç§°"""
    global _labels_cache
    if _labels_cache is None:
        try:
            _labels_cache = np.load(os.path.join(PREPROCESS_DIR, "label_encoder.npy"), allow_pickle=True)
        except:
            _labels_cache = np.array(["Benign", "DoS_Hulk", "PortScan", "DDoS", "BruteForce", "Unknown"])
    if isinstance(_labels_cache, np.ndarray):
        labels = _labels_cache.tolist()
    else:
        labels = _labels_cache
    if 0 <= idx < len(labels):
        return str(labels[idx])
    return f"Class_{idx}"

def resolve_normal_label(label_list):
    """è§£ææ­£å¸¸æµé‡æ ‡ç­¾åç§°"""
    # å¤„ç†numpyæ•°ç»„ï¼šå…ˆè½¬æ¢ä¸ºåˆ—è¡¨
    if isinstance(label_list, np.ndarray):
        label_list = label_list.tolist()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºï¼ˆè½¬æ¢ä¸ºåˆ—è¡¨åæ£€æŸ¥ï¼‰
    if not label_list or len(label_list) == 0:
        return "Benign"
    
    # æŸ¥æ‰¾æ­£å¸¸æ ‡ç­¾
    candidates = ["benign", "normal", "benign traffic", "normal traffic", "æ­£å¸¸", "0"]
    for cand in candidates:
        for label in label_list:
            label_str = label if isinstance(label, str) else str(label)
            if label_str.lower() == cand:
                return label
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ ‡ç­¾
    return label_list[0] if len(label_list) > 0 else "Benign"

# å°è¯•å¯¼å…¥ seabornï¼ˆå¯é€‰ï¼Œç”¨äºç¾è§‚çš„çƒ­åŠ›å›¾ï¼‰
try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False
    logger.warning("âš ï¸ seaborn æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ matplotlib åŸºæœ¬å›¾è¡¨")

# å®šä¹‰ TrafficDatasetï¼ˆé¿å…å¯¼å…¥ train_modelï¼Œå› ä¸º train_model åœ¨æ¨¡å—çº§åˆ«ä¼šæ£€æµ‹GPUï¼‰
# è¿™ä¸ªç±»ä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„ TrafficDataset å®Œå…¨ä¸€è‡´
class TrafficDataset:
    """æ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½æ—¶åºæ•°æ®ï¼ˆä¸ train_model.py ä¸­çš„ TrafficDataset ä¸€è‡´ï¼‰"""
    def __init__(self, X, y, seq_len=SEQ_LEN):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)
        self.seq_len = seq_len

    def __len__(self):
        return len(self.X) - self.seq_len + 1

    def __getitem__(self, idx):
        x_seq = self.X[idx:idx + self.seq_len]
        y_label = self.y[idx + self.seq_len - 1]
        return x_seq, y_label

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - [è¯„ä¼°æ—¥å¿—] - %(message)s")
logger = logging.getLogger(__name__)

def load_evaluation_data(data_dir=PREPROCESS_DIR):
    """åŠ è½½è¯„ä¼°æ‰€éœ€çš„æ•°æ®æ–‡ä»¶"""
    global _labels_cache
    data_dir = Path(data_dir)
    try:
        X_test = np.load(data_dir / "X_test.npy")
        y_test = np.load(data_dir / "y_test.npy")
        labels = np.load(data_dir / "label_encoder.npy", allow_pickle=True)
        _labels_cache = labels  # ç¼“å­˜æ ‡ç­¾ä¾› get_label_name ä½¿ç”¨
        normal_label = resolve_normal_label(labels)
        logger.info(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®ï¼šX_test.shape={X_test.shape}, y_test.shape={y_test.shape}")
        logger.info(f"   æ”¯æŒçš„æ ‡ç­¾ï¼š{list(labels)}")
        return X_test, y_test, labels
    except FileNotFoundError as e:
        logger.error(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{e}")
        raise SystemExit(1)

def evaluate_model(model, X_test, y_test, labels, normal_label, scaler=None, pca=None):
    """ä½¿ç”¨æµ‹è¯•æ•°æ®è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    logger.info(f"   è¾“å…¥æ•°æ®å½¢çŠ¶ï¼šX_test.shape={X_test.shape}")
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç»´åº¦ï¼šå¦‚æœX_testæ˜¯16ç»´ï¼Œéœ€è¦é¢„å¤„ç†ï¼›å¦‚æœæ˜¯12ç»´ï¼Œå·²ç»æ˜¯PCAé™ç»´åçš„æ•°æ®
    if X_test.shape[1] == 16 and scaler is not None and pca is not None:
        logger.info("   æ£€æµ‹åˆ°16ç»´åŸå§‹ç‰¹å¾ï¼Œä½¿ç”¨scalerå’Œpcaè¿›è¡Œé¢„å¤„ç†...")
        X_test_scaled = scaler.transform(X_test)
        X_test_pca = pca.transform(X_test_scaled)
        logger.info(f"   é¢„å¤„ç†åæ•°æ®å½¢çŠ¶ï¼šX_test_pca.shape={X_test_pca.shape}")
        X_test = X_test_pca
    elif X_test.shape[1] == 12:
        logger.info("   æ£€æµ‹åˆ°12ç»´PCAé™ç»´æ•°æ®ï¼Œè·³è¿‡é¢„å¤„ç†æ­¥éª¤")
    else:
        logger.warning(f"   æ•°æ®ç»´åº¦å¼‚å¸¸ï¼šX_test.shape={X_test.shape}ï¼ŒæœŸæœ›12ç»´ï¼ˆPCAåï¼‰æˆ–16ç»´ï¼ˆåŸå§‹ç‰¹å¾ï¼‰")
    
    model.eval()
    
    # æ„å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    test_dataset = TrafficDataset(X_test, y_test)
    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)
    
    all_predictions = []
    all_probabilities = []
    all_true_labels = []
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªbatchçš„æ•°æ®æ ¼å¼
    first_batch_checked = False
    
    with torch.no_grad():
        for batch_x, batch_y in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
            batch_x = batch_x.to(DEVICE)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªbatchçš„æ•°æ®æ ¼å¼
            if not first_batch_checked:
                logger.info(f"   ç¬¬ä¸€ä¸ªbatchæ•°æ®å½¢çŠ¶ï¼šbatch_x.shape={batch_x.shape}")
                logger.info(f"   ç¬¬ä¸€ä¸ªbatchæ ‡ç­¾åˆ†å¸ƒï¼š{np.bincount(batch_y.cpu().numpy())}")
                first_batch_checked = True
            
            # model å°±æ˜¯ discriminator æœ¬èº«ï¼Œç›´æ¥è°ƒç”¨
            _, class_pred = model(batch_x)
            batch_pred = class_pred.argmax(1).cpu().numpy()
            batch_probs = torch.softmax(class_pred, dim=1).cpu().numpy()
            
            all_predictions.extend(batch_pred)
            all_probabilities.extend(batch_probs)
            all_true_labels.extend(batch_y.cpu().numpy())
    
    y_pred = np.array(all_predictions)
    y_true = np.array(all_true_labels)
    all_probabilities = np.array(all_probabilities)
    
    logger.info("æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œå¼€å§‹è®¡ç®—æŒ‡æ ‡...")
    logger.info(f"   é¢„æµ‹ç»“æœåˆ†å¸ƒï¼š{np.bincount(y_pred)}")
    logger.info(f"   çœŸå®æ ‡ç­¾åˆ†å¸ƒï¼š{np.bincount(y_true)}")
    logger.info(f"   é¢„æµ‹æ¦‚ç‡ç»Ÿè®¡ï¼šmin={all_probabilities.min():.4f}, max={all_probabilities.max():.4f}, mean={all_probabilities.mean():.4f}")
    
    # 1. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ (Precision, Recall, F1)
    # è·å–æ‰€æœ‰å®é™…å‡ºç°çš„ç±»åˆ«ï¼ˆåŒ…æ‹¬é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ï¼‰
    all_unique_labels = np.unique(np.concatenate([y_true, y_pred]))
    max_class_idx = max(all_unique_labels.max(), len(labels) - 1) if len(labels) > 0 else all_unique_labels.max()
    num_classes_actual = len(all_unique_labels)
    num_classes_model = len(labels)
    
    logger.info(f"   å®é™…å‡ºç°çš„ç±»åˆ«æ•°ï¼š{num_classes_actual}ï¼Œæœ€å¤§ç±»åˆ«ç´¢å¼•ï¼š{max_class_idx}ï¼Œæ¨¡å‹æ ‡ç­¾æ•°ï¼š{num_classes_model}")
    
    # æ„å»ºå®Œæ•´çš„æ ‡ç­¾åç§°åˆ—è¡¨ï¼ˆè¦†ç›–æ‰€æœ‰å¯èƒ½å‡ºç°çš„ç±»åˆ«ç´¢å¼•ï¼‰
    default_labels = ["Benign", "DoS_Hulk", "DoS_GoldenEye", "PortScan", "DDoS", "BruteForce"]
    
    # æ‰©å±•æ ‡ç­¾åˆ—è¡¨ä»¥è¦†ç›–æ‰€æœ‰å®é™…å‡ºç°çš„ç±»åˆ«
    extended_labels = list(labels) if isinstance(labels, (list, np.ndarray)) else []
    
    # å¦‚æœæ ‡ç­¾åˆ—è¡¨ä¸å¤Ÿé•¿ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾è¡¥å……
    while len(extended_labels) <= max_class_idx:
        if len(extended_labels) < len(default_labels):
            extended_labels.append(default_labels[len(extended_labels)])
        else:
            extended_labels.append(f"Class_{len(extended_labels)}")
    
    # ç¡®ä¿æ ‡ç­¾åˆ—è¡¨é•¿åº¦è¶³å¤Ÿ
    target_names = [str(extended_labels[i]) if i < len(extended_labels) else f"Class_{i}" for i in range(max_class_idx + 1)]
    
    # ä½¿ç”¨ labels å‚æ•°æ˜ç¡®æŒ‡å®šæ‰€æœ‰å®é™…å‡ºç°çš„ç±»åˆ«ï¼Œé¿å…ç±»åˆ«æ•°ä¸åŒ¹é…é”™è¯¯
    classification_rep = classification_report(
        y_true, y_pred, 
        labels=list(all_unique_labels),  # åªä½¿ç”¨å®é™…å‡ºç°çš„ç±»åˆ«
        target_names=[target_names[i] for i in all_unique_labels],
        output_dict=True,
        zero_division=0
    )
    
    # 2. ROCæ›²çº¿å’ŒAUCï¼ˆäºŒåˆ†ç±»ï¼šæ”»å‡» vs æ­£å¸¸ï¼‰
    # æŸ¥æ‰¾ normal_label çš„ç´¢å¼•ï¼ˆé€šå¸¸åœ¨ç´¢å¼•0ï¼Œå³"Benign"ï¼‰
    normal_idx = 0  # é»˜è®¤ä½¿ç”¨ç´¢å¼•0ä½œä¸ºæ­£å¸¸æ ‡ç­¾
    for idx, name in enumerate(target_names):
        if str(name).lower() == str(normal_label).lower() or "benign" in str(name).lower():
            normal_idx = idx
            break
    logger.info(f"   æ­£å¸¸æ ‡ç­¾ç´¢å¼•ï¼š{normal_idx}ï¼ˆ{target_names[normal_idx] if normal_idx < len(target_names) else 'Unknown'}ï¼‰")
    y_true_binary = (y_true != normal_idx).astype(int)
    
    # è®¡ç®— y_scoresï¼šå¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œä½¿ç”¨é¢„æµ‹ä¸ºæ”»å‡»ç±»åˆ«çš„æœ€å¤§æ¦‚ç‡
    y_scores = []
    for i in range(len(y_pred)):
        if y_pred[i] != normal_idx:
            # é¢„æµ‹ä¸ºæ”»å‡»ï¼Œä½¿ç”¨å…¶ç½®ä¿¡åº¦
            y_scores.append(all_probabilities[i][y_pred[i]])
        else:
            # é¢„æµ‹ä¸ºæ­£å¸¸ï¼Œä½¿ç”¨1å‡å»æ­£å¸¸ç±»åˆ«çš„æ¦‚ç‡ï¼ˆä½œä¸º"å¼‚å¸¸åˆ†æ•°"ï¼‰
            y_scores.append(1.0 - all_probabilities[i][normal_idx])
    y_scores = np.array(y_scores)
    
    fpr, tpr, _ = roc_curve(y_true_binary, y_scores)
    roc_auc = roc_auc_score(y_true_binary, y_scores)
    
    # 3. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    return {
        "classification_report": classification_rep,
        "roc_auc": roc_auc,
        "fpr": fpr,
        "tpr": tpr,
        "confusion_matrix": cm,
        "labels": target_names,
        "normal_label": normal_label,
        "y_true": y_true,
        "y_pred": y_pred,
        "y_scores": y_scores
    }

def generate_reports(metrics, model_name="WGAN_v1.0", eval_date=None):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ˆJSON å’Œ Markdown æ ¼å¼ï¼‰"""
    if eval_date is None:
        eval_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report_data = {
        "model_version": model_name,
        "evaluation_date": eval_date,
        "dataset": "CICIDS2017 Test Set",
        "test_samples": len(metrics["y_true"]),
        "metrics": {
            "classification": metrics["classification_report"],
            "roc_auc": float(metrics["roc_auc"])
        },
        "privacy_budget": {
            "epsilon": None,
            "delta": None,
            "noise_multiplier": None,
            "training_steps": None,
            "note": "å½“å‰æ¨¡å‹ä¸ºæ ‡å‡†WGANï¼Œæœªä½¿ç”¨Opacusè®­ç»ƒ"
        }
    }
    
    # ä¿å­˜JSONæŠ¥å‘Š
    with open("evaluation_report.json", "w", encoding="utf-8") as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    logger.info("âœ… ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ševaluation_report.json")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    macro_avg = metrics["classification_report"].get("macro avg", {})
    precision = macro_avg.get("precision", 0.0)
    recall = macro_avg.get("recall", 0.0)
    f1_score = macro_avg.get("f1-score", 0.0)
    
    markdown_content = f"""# DP-WGAN æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

**è¯„ä¼°æ—¶é—´ï¼š** {eval_date}
**æ¨¡å‹ç‰ˆæœ¬ï¼š** {model_name}
**æ•°æ®é›†ï¼š** CICIDS2017 æµ‹è¯•é›†
**æµ‹è¯•æ ·æœ¬æ•°ï¼š** {len(metrics["y_true"])}
**æ­£å¸¸æ ‡ç­¾ï¼š** {metrics["normal_label"]}

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### åˆ†ç±»æ€§èƒ½ï¼ˆmacro-averageï¼‰
- **Precisionï¼ˆç²¾ç¡®ç‡ï¼‰ï¼š** {precision:.4f}
- **Recallï¼ˆå¬å›ç‡ï¼‰ï¼š** {recall:.4f}
- **F1-Scoreï¼š** {f1_score:.4f}

### ROCæ›²çº¿å’ŒAUC
- **AUCå€¼ï¼š** {metrics["roc_auc"]:.4f}

### è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
```
{json.dumps(metrics["classification_report"], indent=2, ensure_ascii=False)}
```

## ğŸ“ˆ å¯è§†åŒ–ç»“æœ

### ROCæ›²çº¿
![ROC Curve](roc_curve.png)

### æ··æ·†çŸ©é˜µ
![Confusion Matrix](confusion_matrix.png)

## ğŸ”’ éšç§é¢„ç®—

å½“å‰æ¨¡å‹ä¸ºæ ‡å‡†WGANï¼Œæœªä½¿ç”¨Opacusè®­ç»ƒï¼Œå› æ­¤æ— æ³•è®¡ç®—éšç§é¢„ç®—ï¼ˆÎµ, Î´ï¼‰ã€‚

## ğŸ“ æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯

- **æ¨¡å‹æ–‡ä»¶ï¼š** `best_model_4x5880_max.pth`
- **è®­ç»ƒé…ç½®ï¼š** è§ `train_model.py`
- **è¯„ä¼°è„šæœ¬ï¼š** `evaluate_dp_wgan.py`

**æ³¨æ„ï¼š** å¦‚éœ€å·®åˆ†éšç§è¯„ä¼°ï¼Œè¯·å…ˆå®Œæˆè®­ç»ƒé˜¶æ®µOpacusé›†æˆåå†è¿è¡Œè¯„ä¼°è„šæœ¬ã€‚

---

**æœ€åæ›´æ–°ï¼š** {eval_date}
**é¡¹ç›®æ–‡æ¡£ï¼š** ã€Šæ™ºé“¾åˆ†ææº¯æºå¹³å°ã€‹æ¦‚è¦ä»‹ç»æ–‡æ¡£ v4.0
"""
    
    with open("evaluation_report.md", "w", encoding="utf-8") as f:
        f.write(markdown_content)
    logger.info("âœ… ç”ŸæˆMarkdownæŠ¥å‘Šï¼ševaluation_report.md")

def plot_roc_curve(fpr, tpr, roc_auc):
    """ç»˜åˆ¶ROCæ›²çº¿"""
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("roc_curve.png", dpi=150, bbox_inches='tight')
    plt.close()
    logger.info("âœ… ç”ŸæˆROCæ›²çº¿ï¼šroc_curve.png")

def plot_confusion_matrix(cm, labels):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
    plt.figure(figsize=(10, 8))
    if HAS_SEABORN:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=labels, yticklabels=labels,
                    annot_kws={'size': 10})
    else:
        # ä½¿ç”¨ matplotlib åŸºæœ¬å›¾è¡¨
        plt.imshow(cm, interpolation='nearest', cmap='Blues')
        plt.colorbar()
        tick_marks = np.arange(len(labels))
        plt.xticks(tick_marks, labels, rotation=45)
        plt.yticks(tick_marks, labels)
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                plt.text(j, i, format(cm[i, j], 'd'),
                        horizontalalignment="center",
                        color="white" if cm[i, j] > thresh else "black")
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig("confusion_matrix.png", dpi=150, bbox_inches='tight')
    plt.close()
    logger.info("âœ… ç”Ÿæˆæ··æ·†çŸ©é˜µï¼šconfusion_matrix.png")

def main():
    """ä¸»å‡½æ•° - æ¨¡å‹ç¦»çº¿è¯„ä¼°"""
    parser = argparse.ArgumentParser(description="DP-WGAN æ¨¡å‹ç¦»çº¿è¯„ä¼°")
    parser.add_argument("--model_path", default=os.path.join(MODEL_DIR, "best_model_4x5880_max.pth"),
                        help="æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data_dir", default=PREPROCESS_DIR,
                        help="é¢„å¤„ç†æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output_dir", default="./evaluation_results/",
                        help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    os.chdir(output_dir)
    
    logger.info(f"ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°")
    logger.info(f"ğŸ“ æ•°æ®ç›®å½•ï¼š{args.data_dir}")
    logger.info(f"ğŸ“ æ¨¡å‹è·¯å¾„ï¼š{args.model_path}")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•ï¼š{output_dir}")
    
    # 1. åŠ è½½æ•°æ®
    X_test, y_test, labels = load_evaluation_data(args.data_dir)
    normal_label = resolve_normal_label(labels)
    
    # 2. åŠ è½½æ¨¡å‹
    try:
        discriminator, generator, scaler, pca, model_labels = load_model()
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        logger.info(f"   æ¨¡å‹æ”¯æŒçš„æ ‡ç­¾ï¼š{list(model_labels)}")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. è¿›è¡Œé¢„æµ‹å’Œè¯„ä¼°ï¼ˆä½¿ç”¨ discriminatorï¼‰
    # æ³¨æ„ï¼šX_test.npy åº”è¯¥å·²ç»æ˜¯é¢„å¤„ç†åçš„æ•°æ®ï¼ˆPCAé™ç»´åçš„12ç»´ï¼‰
    # evaluate_model ä¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®ç»´åº¦ï¼Œå†³å®šæ˜¯å¦éœ€è¦é¢„å¤„ç†
    # ä½¿ç”¨æ¨¡å‹æ ‡ç­¾ï¼ˆmodel_labelsï¼‰è€Œä¸æ˜¯æ•°æ®æ ‡ç­¾ï¼Œå› ä¸ºæ¨¡å‹è¾“å‡ºå¯èƒ½åŒ…å«æ›´å¤šç±»åˆ«
    metrics = evaluate_model(discriminator, X_test, y_test, model_labels, normal_label, scaler, pca)
    
    # 4. ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–
    eval_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    generate_reports(metrics, "WGAN_v1.0", eval_date)
    plot_roc_curve(metrics["fpr"], metrics["tpr"], metrics["roc_auc"])
    plot_confusion_matrix(metrics["confusion_matrix"], metrics["labels"])
    
    logger.info("âœ… è¯„ä¼°å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
    logger.info(f"ğŸ“Š æŠ¥å‘Šæ–‡ä»¶ï¼ševaluation_report.json, evaluation_report.md")
    logger.info(f"ğŸ“Š å¯è§†åŒ–æ–‡ä»¶ï¼šroc_curve.png, confusion_matrix.png")

if __name__ == "__main__":
    main()
